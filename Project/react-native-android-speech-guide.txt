# Comprehensive Guide to Implementing react-native-android-speech in React Native Apps

This guide explains how to properly implement the `react-native-android-speech` module in a React Native application, breaking down the architecture, setup, and usage patterns.

## 1. Module Architecture Overview

The `react-native-android-speech` module provides a bridge between React Native and the Android SpeechRecognizer API, allowing apps to implement voice recognition functionality. The implementation consists of:

- **Native Android layer**: Java code that interfaces with Android's SpeechRecognizer
- **JavaScript wrapper**: Provides a clean API for React components
- **Event system**: Communicates recognition states and results between native and JS code

## 2. Installation and Setup

### Installation

```bash
npm install react-native-android-speech --save
# or
yarn add react-native-android-speech
```

### Android Configuration

1. **Add permissions** to your AndroidManifest.xml:

```xml
<uses-permission android:name="android.permission.RECORD_AUDIO" />
```

2. **For local development** (if using a local copy of the module), configure metro.config.js:

```javascript
const path = require('path');
const { getDefaultConfig } = require('expo/metro-config');

const projectRoot = __dirname;
const moduleRoot = path.resolve(projectRoot, '../react-native-android-speech');

const config = getDefaultConfig(projectRoot);

// Add the local module directory to watchFolders
config.watchFolders = [moduleRoot];

// Add custom resolver to handle the local module
config.resolver.nodeModulesPaths = [
  path.resolve(projectRoot, 'node_modules'),
  path.resolve(moduleRoot, 'node_modules')
];

// Force the custom module to be treated as a regular module
config.resolver.extraNodeModules = {
  'react-native-android-speech': moduleRoot
};

module.exports = config;
```

## 3. Permission Handling

Create a permissions helper function:

```javascript
// permissions.js
import { PermissionsAndroid, Platform } from 'react-native';

export const requestMicrophonePermission = async () => {
  if (Platform.OS !== 'android') {
    return true;
  }
  
  try {
    const granted = await PermissionsAndroid.request(
      PermissionsAndroid.PERMISSIONS.RECORD_AUDIO,
      {
        title: "Microphone Permission",
        message: "This app needs access to your microphone for speech recognition",
        buttonNeutral: "Ask Me Later",
        buttonNegative: "Cancel",
        buttonPositive: "OK"
      }
    );
    return granted === PermissionsAndroid.RESULTS.GRANTED;
  } catch (err) {
    console.warn(err);
    return false;
  }
};
```

## 4. Basic Implementation in a Component

```javascript
import React, { useState, useEffect } from 'react';
import { View, Text, TouchableOpacity, Platform } from 'react-native';
import AndroidSpeech from 'react-native-android-speech';
import { requestMicrophonePermission } from './permissions';

export default function VoiceRecognitionComponent() {
  const [isListening, setIsListening] = useState(false);
  const [results, setResults] = useState('');
  const [partialResults, setPartialResults] = useState('');
  const [isAvailable, setIsAvailable] = useState(false);
  const [error, setError] = useState('');

  useEffect(() => {
    // Check if speech recognition is available
    const checkAvailability = async () => {
      if (Platform.OS === 'android') {
        const available = await AndroidSpeech.checkAvailability();
        setIsAvailable(available);
        if (!available) {
          setError('Speech recognition is not available on this device');
        }
      } else {
        setError('This feature is only available on Android');
      }
    };

    checkAvailability();

    // Set up event listeners
    if (Platform.OS === 'android') {
      const startListener = AndroidSpeech.addListener(
        AndroidSpeech.events.START,
        () => console.log('Speech recognition started')
      );

      const endListener = AndroidSpeech.addListener(
        AndroidSpeech.events.END,
        () => {
          console.log('Speech recognition ended');
          setIsListening(false);
        }
      );

      const resultsListener = AndroidSpeech.addListener(
        AndroidSpeech.events.RESULTS,
        (event) => {
          console.log('Speech results:', event.value);
          setResults(event.value);
          setPartialResults('');
        }
      );

      const errorListener = AndroidSpeech.addListener(
        AndroidSpeech.events.ERROR,
        (event) => {
          console.error('Speech recognition error:', event.error);
          setError(`Error: ${event.error}`);
          setIsListening(false);
        }
      );

      const partialResultsListener = AndroidSpeech.addListener(
        AndroidSpeech.events.PARTIAL_RESULTS,
        (event) => {
          console.log('Partial results:', event.value);
          setPartialResults(event.value);
        }
      );

      // Cleanup listeners on unmount
      return () => {
        startListener.remove();
        endListener.remove();
        resultsListener.remove();
        errorListener.remove();
        partialResultsListener.remove();
      };
    }
  }, []);

  const startSpeechRecognition = async () => {
    setError('');
    
    if (Platform.OS !== 'android') {
      setError('This feature is only available on Android');
      return;
    }

    if (!isAvailable) {
      setError('Speech recognition is not available on this device');
      return;
    }

    // Request microphone permission
    const hasPermission = await requestMicrophonePermission();
    if (!hasPermission) {
      setError('Microphone permission denied');
      return;
    }

    try {
      await AndroidSpeech.start({
        language: 'en-US', // You can change this to other languages
      });
      setIsListening(true);
      setPartialResults('');
    } catch (error) {
      console.error('Error starting speech recognition:', error);
      setError(`Failed to start: ${error.message}`);
    }
  };

  const stopSpeechRecognition = async () => {
    if (Platform.OS === 'android' && isListening) {
      try {
        await AndroidSpeech.stop();
        setIsListening(false);
      } catch (error) {
        console.error('Error stopping speech recognition:', error);
        setError(`Failed to stop: ${error.message}`);
      }
    }
  };

  const clearResults = () => {
    setResults('');
    setPartialResults('');
    setError('');
  };

  return (
    <View>
      {error ? (
        <Text>{error}</Text>
      ) : (
        <>
          {results ? (
            <Text>{results}</Text>
          ) : partialResults ? (
            <Text>{partialResults}</Text>
          ) : (
            <Text>Tap the microphone to start speech recognition</Text>
          )}
        </>
      )}

      <TouchableOpacity 
        onPress={clearResults}
      >
        <Text>Clear</Text>
      </TouchableOpacity>

      <TouchableOpacity
        onPress={isListening ? stopSpeechRecognition : startSpeechRecognition}
        disabled={!isAvailable && Platform.OS === 'android'}
      >
        <Text>{isListening ? 'Stop' : 'Start'}</Text>
      </TouchableOpacity>
    </View>
  );
}
```

## 5. Understanding the Native Module Structure

The module consists of these key components:

### Java Implementation

- **AndroidSpeechPackage.java**: Registers the module with React Native
- **AndroidSpeechRecognitionModule.java**: Core implementation that:
  - Creates and manages the SpeechRecognizer instance
  - Handles the RecognitionListener callbacks
  - Maps Android events to JavaScript events
  - Provides methods for starting/stopping recognition

### JavaScript API

The index.js file provides:

```javascript
// Key API methods:
AndroidSpeech.checkAvailability() // Returns Promise<boolean>
AndroidSpeech.start({ language }) // Returns Promise<void>
AndroidSpeech.stop()             // Returns Promise<void>
AndroidSpeech.isRecognizing()    // Returns Promise<boolean>

// Event listeners:
AndroidSpeech.addListener(eventName, callback)
AndroidSpeech.removeAllListeners(eventName)

// Events constants:
AndroidSpeech.events.START          // Recognition started
AndroidSpeech.events.END            // Recognition ended
AndroidSpeech.events.RESULTS        // Final results available
AndroidSpeech.events.ERROR          // Error occurred
AndroidSpeech.events.PARTIAL_RESULTS // Intermediate results
```

## 6. Best Practices

### Error Handling

Always implement proper error handling for speech recognition:

```javascript
try {
  await AndroidSpeech.start({ language: 'en-US' });
} catch (error) {
  // Handle specific error cases
  if (error.message.includes('SPEECH_NOT_AVAILABLE')) {
    // Speech recognition not available on this device
  } else if (error.message.includes('PERMISSION_DENIED')) {
    // Permission issues
  } else if (error.message.includes('ALREADY_RECOGNIZING')) {
    // Already in progress
  } else {
    // General error handling
  }
}
```

### Lifecycle Management

- Properly clean up event listeners in useEffect return function
- Stop recognition when component unmounts
- Handle app background/foreground transitions

### UI Integration

- Provide clear visual feedback about recognition states
- Display partial results for better UX
- Implement error messages that guide the user

### Performance Considerations

- Don't start recognition unnecessarily
- Be mindful of battery usage
- Implement timeouts for long recognition sessions

## 7. Advanced Usage

### Customizing Language

```javascript
await AndroidSpeech.start({ language: 'fr-FR' }); // French
await AndroidSpeech.start({ language: 'es-ES' }); // Spanish
await AndroidSpeech.start({ language: 'de-DE' }); // German
```

### Handling Different Error Codes

The module exposes error constants that can be used for specific error handling:

```javascript
AndroidSpeech.addListener(AndroidSpeech.events.ERROR, (event) => {
  switch (event.code) {
    case AndroidSpeech.ERROR_NETWORK:
      // Handle network error
      break;
    case AndroidSpeech.ERROR_NO_MATCH:
      // Handle no recognition match
      break;
    case AndroidSpeech.ERROR_SPEECH_TIMEOUT:
      // Handle timeout
      break;
    // etc.
  }
});
```

## 8. Troubleshooting Common Issues

- **Recognition not starting**: Check permissions and availability
- **No results returned**: Verify microphone is working and not blocked
- **Crashes when starting**: Ensure proper initialization and permission handling
- **Unexpected behavior on specific devices**: Test on various Android versions

By following this guide, you can successfully integrate the react-native-android-speech module into your React Native application to provide robust speech recognition functionality.